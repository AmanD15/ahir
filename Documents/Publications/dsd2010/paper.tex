\documentclass[conference]{IEEEtran} 
\usepackage{graphicx}
\usepackage[tight,footnotesize]{subfigure}
\usepackage{amsmath}
\usepackage{alltt}
\usepackage{multirow}
\usepackage{times}
\usepackage{url}

\title{A C-to-RTL flow as an energy efficient alternative to embedded
  processors in digital systems}

\author{Sameer D. Sahasrabuddhe, Sreenivas
  Subramanian, Kunal P. Ghosh, Kavi Arya and Madhav P. Desai\\
  Indian Institute of Technology -- Bombay, Powai, Mumbai -- 400076, INDIA\\
  Email: \{sameerds@cse, ssreenivas@ee, kunalghosh@ee, kavi@cse,
  madhav@ee\}.iitb.ac.in }
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\newcommand{\sym}[1]{$\operatorname{#1}$}

\newbox\subfigbox             % Create a box to hold the subfigure.
\makeatletter
  \newenvironment{subfloat}% % Create the new environment.
    {\def\caption##1{\gdef\subcapsave{\relax##1}}%
     \let\subcapsave=\@empty % Save the subcaption text.
     \let\sf@oldlabel=\label
     \def\label##1{\xdef\sublabsave{\noexpand\label{##1}}}%
     \let\sublabsave\relax    % Save the label key.
     \setbox\subfigbox\hbox
       \bgroup}%              % Open the box...
      {\egroup                % ... close the box and call \subfigure.
     \let\label=\sf@oldlabel
     \subfigure[\subcapsave]{\box\subfigbox\sublabsave}}%
\makeatother

\pagestyle{empty}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}

  We present a high-level synthesis flow for mapping an algorithm
  description (in C) to a provably equivalent register-transfer level
  (RTL) description of hardware. This flow uses an intermediate
  representation which is an orthogonal factorization of the program
  behavior into control, data and memory aspects, and is suitable for
  the description of large systems. We show that optimizations such as
  arbiter-less resource sharing can be efficiently computed on this
  representation. We apply the flow to a wide range of examples
  ranging from stream ciphers to database and linear algebra
  applications. The resulting RTL is then put through a standard ASIC
  tool chain (synthesis followed by automatic place-and-route), and
  the performance and power dissipation of the resulting layout is
  computed. We observe that the energy consumption (per completed
  task) of each resulting circuit is considerably lower than that of
  an equivalent executable running on a low-power processor,
  indicating that this C-to-RTL flow offers an energy efficient
  alternative to the use of embedded processors in mapping algorithms
  to digital VLSI systems.

\end{abstract}

\section{Introduction}


The design of complex digital systems is expensive due to two reasons:
the need for trained manpower, and the difficulty of verification at
every step of the design process. Thus, it is often the case that
low-power embedded microprocessors are used to implement complex
algorithms in digital systems, so that the design and verification
problems are moved to the software domain. A high-level synthesis flow
which maps an algorithm directly to a hardware description can be a
potential alternative if it provides a verifiable and optimisable path
from executable specifications\cite{gajski-executable-specification}
to hardware.   

In addition to the ease of use, performance measures and energy
considerations are standards by which a high-level synthesis flow
should be judged relative to the competing methodologies --- custom
designed RTL and embedded processors.  It is very critical
that the results of the high-level synthesis flow are characterized
with respect to their area/power/delay/energy, both in absolute terms
and relative to competing methodologies.  As far as comparisons
with  custom designed hardware are concerned, a high-level synthesis flow
which starts from a C/C++ description of the algorithm is unlikely to
be competitive, mainly because the amount of parallelism expressible in 
a C/C++ program is not sufficient.   It should be possible to address
this shortcoming by starting with algorithms described in parallel
programming languages, but such discussions are beyond the scope of
this paper.   

We provide comparisons between
the hardware generated by our high level synthesis flow and  
processor implementations of an algorithm described in C.  
Note that it is not entirely obvious that algorithm-specific hardware 
generated by a high-level synthesis flow will be superior
to embedded processor implementations of the same algorithm.  
One can argue that the algorithm-specific hardware
can exploit the characteristics of the algorithm to a greater
degree than a general purpose processor.  
On the other hand, when a C/C++ program is compiled to
a processor, the platform (that is, the processor) is usually a
heavily optimized circuit in which each functional
element has been tweaked to a
high level, whereas the algorithm-specific
circuit is generated by automated tools whose optimization abilities
will most probably not match those of a human designer.   
Finally, since it is not possible to express parallelism
directly in a C program (although a compiler can infer it),
it is not too clear whether a high-level synthesis system can
identify enough parallelism to make the resulting hardware competitive
with a heavily optimized processor.  Concrete data which throws light on these aspects is valuable.

Our high-level synthesis flow is based on a factored intermediate
representation which guarantees the correctness of the implementation,
and supports optimisations that can scale with the size of the
program. The synthesis flow introduces an intermediate step in the
form of a representation called AHIR (A Hardware Intermediate
Representation). The representation is independent of the programming
language used, and can be routinely translated to a hardware
implementation, while also supporting scalable
optimisations\cite{AHIR_sameerds_hakim_2007}\cite{ahir_thesis}.

In order to characterize the energy efficiency of the resulting
circuits, we select a wide range of applications (from stream ciphers
to databases and linear algebra) and run them through our C-to-RTL
flow. The resulting RTL is then mapped to an ASIC using industry
standard automatic synthesis tools. This ASIC is characterized for
power, delay, area and energy, and the results are compared with those
obtained when the same program is mapped to a low power processor.
The results indicate that even in its current state, our high-level
synthesis flow is an energy competitive alternative to the use of
embedded processors in the design of complex systems.


\section{Related work}

There has been a great deal of activity in the field of 
high-level synthesis in the context of converting a high
level program to RTL.   Attempts at creating a path from 
high-level programs to hardware descriptions can be loosely 
categorised as follows:

\subsection{Improvements over RTL}

Efforts such as Bluespec\cite{bluespec-memo} raise the abstraction in an
RTL description in order to support higher-level constructs. Such a
language can be very powerful in expressing the architecture of the
hardware, but the target user is a hardware designer who can
effectively utilize this expressive power.

\subsection{Modified high-level languages}

Some efforts reinterpret programming languages as hardware
descriptions, and also extend them with special features. The language
SA-C\cite{SA-C}, for example, is a purely
functional subset of C that disallows pointers. On the other hand,
Handel-C\cite{handel-c} is a language that guarantees complete ISO-C
compatibility and also provides additional primitives.

In both examples, the designer must use specific features to generate
efficient hardware, instead of the compiler inferring a hardware
implementation. This forces the programmer to reevaluate standard
programming practices.


\subsection{High-level programs as hardware specifications}

Some efforts simply interpret a program as a behavioral
specification, which is mapped to a hardware implementation using an
intermediate representation.

For example, the Phoenix project uses an intermediate representation
called Pegasus\cite{pegasus-ir} for a compiler flow from C to
hardware\cite{pegasus-cash}. A description in Pegasus can be
implemented as a micro-pipeline. The representation allows the compiler
to natively implement a number of high-level transformations.

The SPARK\cite{spark-vlsi-paper} project uses an internal
representation based on hierarchical task graphs (HTG). The compiler
uses a heuristic to combine high-level transformations on the HTG ---
such as code motion and speculation\cite{spark-code-motion} --- with
scheduling and resource binding to produce efficient hardware.

Our work is similar, since the goal is to transparently compile
programs into hardware. AHIR differs from both Pegasus and SPARK since
it factorises the system into three separate components: control flow,
data flow and memory.  The components can be optimised and
implemented separately as long as specified constraints are satisfied.

Several commercial tools which translate a C program to hardware are
also available. For example,
Catapult-C\footnote{\url{http://www.mentor.com/products/esl/high_level_synthesis/}}
from Mentor Graphics,
C-to-Silicon\footnote{\url{http://www.cadence.com/products/sd/silicon_compiler/}}
from Cadence, and Synopsys
HLS\footnote{\url{http://www.synopsys.com/Tools/SLD/HLS/}} from
Synopsys. Currently, we have no access to the internal details of
these proprietary tools.


\section{AHIR}

A system in AHIR is a collection of modules connected to a memory
subsystem as shown in Fig.~\ref{fig:system}. Each module represents
one function from the input program. Function calls are implemented
through an \emph{inter-module link layer}. The architecture of the
memory subsystem is not defined in AHIR. It is only required to
service every request \emph{eventually}.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.2]{images/omega.eps}
  \caption{An AHIR system.}
  \label{fig:system}
\end{figure}

\subsection{A module in AHIR}


\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.25]{images/decouple.eps}
  \caption{An AHIR module.}
  \label{fig:decouple}
\end{figure}

Two flows are described in an AHIR module --- control-path and
data-path --- that communicate through an \emph{intra-module link
  layer} as shown in Fig.~\ref{fig:decouple}. The control-path is a
petri-net that specifies the ordering of events in the module. The
data-path is a pool of hardware resources connected by wires.

Communication through the link layers is specified as an exchange of
symbols. The set of symbols associated with a component is called its
alphabet. The data-path uses alphabet $\Sigma$, while the control-path
uses alphabet $\Lambda$. The interaction with the inter-module link
layer is represented by the alphabet $\Omega$.

\subsection{Data-path}

The data-path is a directed hyper-graph, where the edges represent
values, and the nodes represent operations on these values. Each edge
is a hyper-edge with a single tail and one or more heads. The tail
drives a value on the edge, which reaches all the heads
instantaneously.

A data-path node is described by a state machine with an \sym{idle}
state, and one or more \sym{busy} states. When a request
$\operatorname{req}_i$ arrives at an idle node, the node samples all
its incoming data-edges and changes state to $\operatorname{busy}_i$.
The node then operates on the sampled values and updates the outgoing
data-edges. On completion, the node emits an acknowledge symbol
$\operatorname{ack}_j$, and then returns to the \sym{idle} state.

The data-path uses load and store operators to communicate with
external memory. Additionally, there are input and output ports that
are used to exchange arguments and return values with the inter-module
link layer during a function call.

\subsection{Control-path}
\index{control-path}

%\begin{figure}
%  \centering
%  \includegraphics[scale=0.25]{images/control-path-example.eps}
%  \caption{An AHIR Control-path.}
%  \label{fig:control-path-example}
%\end{figure}

The control-path is a petri-net that expresses the sequence in which
events occur in a module. It is required to be in a class called
``Type-2 Petri-nets'', as defined in Section~\ref{section:type-2}.
The circuit-level implementation paradigm to be used for an AHIR module is
not specified (the implementation may be synchronous or asynchronous).
Thus, petri-nets provide a neutral, powerful and compact mechanism to 
describe parallelism in the control flow of an algorithm.  
As we shall see in Section~\ref{section:type-2}, a ``Type-2'' petri-net
provides an easily verifiable, yet powerful enough mechanism to
describe control flow.

The transitions in the control path are associated with input and
output symbols.  An {\em input transition} is associated with
an input symbol (that is, the transition is assumed to have fired
when the input symbol is received by the control path), and an {\em output transition}
is associated with an output symbol (the output symbol is emitted
when the transition fires).

The control-path has a single marked place in the initial marking.
This enables a single transition called \sym{init}, which responds to
a request symbol in alphabet $\Omega$ and begins execution of the
module. The end of execution is represented by the \sym{fin}
transition that emits an acknowledgment symbol in $\Omega$ and marks
the initially marked place.

\subsection{The Intra-module Link Layer}
\index{link layer!intra-module}

The intra-module link layer translates symbols generated by the
control-path (in $\Lambda$) to symbols for the data-path (in $\Sigma$)
or the inter-module link layer (in $\Omega$), and \emph{vice versa}.
It is defined as a set of functions that instantaneously consume
symbols presented to them and generate new symbols.

\subsection{Handshakes and delay constraints}
\label{sect:handshakes}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.25]{images/isochron.eps}
\caption{Delays in an AHIR specification.}
\label{fig:isochron}
\end{figure}

Operations in AHIR are managed by symbolic handshakes. The
control-path emits a \sym{request} in $\Lambda$, which triggers an
operator in the data-path. When done, the operator emits an
\sym{acknowledge} in $\Sigma$, which causes further events in the
control-path. This \emph{request-acknowledge handshake} encapsulates
any delays in the implementation.

An implementation must satisfy a pair of one-sided delay constraints
for the handshake in order to ensure correctness.
Fig.~\ref{fig:isochron} shows a hypothetical example with associated
delays. When the control-path emits the request symbol \sym{Req}, it
must update its state before the arrival of the acknowledgment symbol
\sym{Ack}. Hence we have:
\[d_5 \le d_0 + d_1 + d_3\]

Similarly, when the data-path emits an \sym{Ack}, it eventually
receives a \sym{Req}. The data inputs must have stabilized before
this request arrives. Hence we have:
\[d_2 \le d_3 + d_4 + d_0\]

Note that the term $d_0 + d_3$ is common to both expressions. An
implementation can always guarantee timing correctness by sufficiently
padding these delays to satisfy the constraints.

\subsection{Execution model}
\index{execution!AHIR module}

AHIR uses the following execution model. The control-path responds
instantaneously to the arrival of symbols, while data-path elements
take some finite non-zero amount of time to execute. Values in the data-path are also
propagated instantaneously. Clearly, this satisfies the delay
constraints, since $d_1$ is finite, while other delays are zero.  
In principle, RTL implementations of AHIR can be synchronous or asynchronous.  In 
our current flow however, we generate synchronous RTL implementations from
AHIR.

\subsection{The Inter-module Link Layer}
\index{link layer!inter-module}

The inter-module link layer is used to route function calls between
modules. It has one arbiter for each module, that manages the flow of
input arguments and return values between the calling module and the
called module. 

\section{Type-2 petri-nets}
\label{section:type-2}

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.2]{images/TPR.eps}
  \caption{A TPR and a Type-1 petri-net}
  \label{figure:TPR}
\end{figure}

\begin{figure}[!t]
  \centering
  \subfigure[Primitive]{
    \label{figure:primitive-TPRs}
    \includegraphics[scale=0.2]{images/Type-2-terminals.eps}}
  \subfigure[Series]{
    \label{figure:series-TPR}
    \hspace{0.125in}
    \includegraphics[scale=0.2]{images/Type-2-linear.eps}
    \hspace{0.125in}}
  \subfigure[Fork]{
    \label{figure:fork-TPR}
    \includegraphics[scale=0.15]{images/Type-2-forks.eps}}
  \hspace{0.125in}
  \subfigure[Branch]{
    \label{figure:branch-TPR}
    \includegraphics[scale=0.15]{images/Type-2-branches.eps}}
  \subfigure[Parallel merges]{
    \label{figure:merge-region}
    \includegraphics[scale=0.2]{images/merge-region.eps}}
  \caption{Type-2 construction rules.}
  \label{figure:Type-2-TPRs}
\end{figure}

Control paths in AHIR are instances of Type-2 petri-nets (introduced
in \cite{ahir_thesis}), which
are defined using a set of standard construction rules. 
The structure of a Type-2 petri-net is
designed to enable analyses and transformations that are scalable with
the size of the petri-net.

\begin{definition} A {\bf simple place (transition)} is a place
(transition) with one incoming edge and one outgoing edge.
\end{definition}

\begin{definition} A {\bf token-preserving region (TPR)} is a
petri-net $P$ that can be augmented with one simple place $\hat{p}$
and a sufficient number of simple transitions and edges, to produce a
live and safe petri-net $P'$ such that $\hat{p}$ is the only marked
place in the initial marking (as shown in Fig.~\ref{figure:TPR}).
\label{definition:TPR}
\end{definition}

\begin{definition} A {\bf Type-1 Petri-net} is a live and safe
petri-net that marks one simple place in the initial marking. Clearly,
a Type-1 petri-net is constructed by augmenting a TPR.
\end{definition}

\begin{definition}
A {\bf Standard TPR (STPR)} is a TPR constructed using the standard
set of rules (defined below).
\end{definition}

\begin{definition} A {\bf Type-2 petri-net} is a Type-1 petri-net
created by augmenting a Standard TPR.
\end{definition}

\subsection*{Type-2 construction rules:}

\begin{enumerate}
  \item A simple place or transition is a {\em primitive} STPR.

  \item A {\em series region} is an STPR formed by joining two
        standard STPRs in series.

  \item A connected {\it acyclic} sub-graph made of STPRs, forks and
        joins is itself an STPR called a {\em fork region}.

  \item A connected (possibly cyclic) sub-graph made of STPRs, branches
        and merges is an STPR called a {\em branch region}.

  \item Replacing a merge place in a branch region with parallel
        merges as shown in Fig.~\ref{figure:merge-region} also
        results in a standard TPR. The set of parallel merges
        introduced by this replacement is called a {\em parallel-merge
        region}.
\end{enumerate}

The Type-2 construction rules are illustrated in
Fig.~\ref{figure:Type-2-TPRs}. The branch region allows cycles in
order to express arbitrary branch and loop structures. The fork region
does not allow cycles since that introduces further conditions for
liveness. But this does not affect the expressive power of Type-2
petri-nets. Parallel-merge regions implement the run-time selection of
values at the exit of a branch, such as variable {\tt d} in
Fig.~\ref{figure:C-and-SSA}.

\section{Arbiter-less sharing of data-path operators}

We describe an optimization that reuses a data-path operator for
multiple operations that are never active at the same time. This
avoids arbitration overheads since there is no contention. The
optimisation uses an almost linear static analysis of the control-path
to identify sharing opportunities.

\begin{definition} An operator is said to be {\bf active} at a given
instant of time if and only if it has received a request, but not
yet emitted an acknowledgment.
\end{definition}

\begin{definition} Two operators $M_1$ and $M_2$ are said to be {\bf
compatible} if and only if $M_1$ does not receive a request while
$M_2$ is active, and {\it vice versa}.
\end{definition}

\subsection{Compatibility in Type-2 petri-nets}

In a Type-2 petri-net, compatibility of two transitions is determined
by the nature of the smallest STPR that contains them, which we term
as their {\em nearest common ancestor} (NCA). Two operations can
potentially be incompatible only if the NCA is a fork region;
operations in a branch or series NCA region are always compatible.

\begin{figure}[!t]
  \centering
  \subfigure[A fork region]{
    \label{figure:compatibility-fork-paths}
    \includegraphics[scale=0.225]{images/compatibility-fork-paths.eps}}
  \subfigure[Compatibility]{
    \begin{tabular}[b]{|c|ccccc|}
      \hline
      & 1 & 2 & 3 & 4 & 5\\
      \hline
      1 & Y & - & Y & Y & Y\\
      2 & - & Y & - & Y & -\\
      3 & Y & - & Y & Y & -\\
      4 & Y & Y & Y & Y & -\\
      5 & Y & - & - & - & Y\\
      \hline
    \end{tabular}
    \label{figure:compatibility-fork-table}}
  \caption{Compatibility in a Type-2 petri-net.}
  \label{figure:compatibility-fork}
\end{figure}

Fig.~\ref{figure:compatibility-fork} shows a fork region in a Type-2
petri-net with numbered segments and their compatibility with each
other. Segments $1$ and $2$ are \emph{incompatible} since they can
execute concurrently. But segments $1$ and $4$ are compatible, since a
sequence is enforced by the path through segment $3$.

\begin{definition} Two elements $e_1$ and $e_2$ in a Type-2 petri-net
are compatible if and only if one of the following is true:
\begin{enumerate}
\item their NCA is {\em not} a fork region
\item there is a path within the NCA fork region, joining the two elements
\end{enumerate}
\end{definition}

\subsection{Compatibility labeling}

We use a labeling scheme to record the paths reaching a petri-net
element from the \sym{init} transition. The labeling is a symbolic
execution of the Type-2 petri-net. Two elements can be compared for
compatibility using their labels instead.

A label is a set $L = \{l_0,l_1,\ldots\}$ where each $l \in L$ is a
sequence of $n$ label elements $l = [a_0,a_1,\ldots,a_{n-1}]$. A label
element is a 3-tuple $(f,k,i)$ made of a fork identifier $f$, the
fan-out $k$ of the fork, and an index $i$ into the fan-out. A label
element $a = (f,k,i)$ is said to {\it indicate} the fork $f$.

The product of two labels $(A * B)$ is
defined as the concatenation of pairs of sequences from the two
labels: $A*B = \{a.b |\forall a \in A, \forall b \in B \}$. For
convenience, the product operator is overloaded to represent the
product of a label with a single element: $A*b = A*\{[b]\}$.

\subsection{Labeling scheme}

Parallel-merge regions are first reduced to simple merges, which
simplifies the labeling without affecting compatibility. The $init$
transition is assigned an empty label. The label of every element is
computed from its predecessors. Only forks and joins result in a new
label; other elements are assigned the same label as their
predecessors.

\subsubsection{Labeling at a fork}

If $L$ is the label assigned to a fork $f$ with $k(f)$ successors,
then each successor $s_i$ is assigned the label $L *
\bigl(f,k(f),i\bigr)$, as shown in Fig.~\ref{figure:fork-labels}.

\begin{figure}[!t]
  \centering
  \subfigure[Fork.]{
    \label{figure:fork-labels}
    \includegraphics[scale=0.25]{images/fork-label-products.eps}}
  \subfigure[Union at a join.]{
    \label{figure:join-union}
    \hspace{0.125in}
    \includegraphics[scale=0.25]{images/join-union-labels.eps}
    \hspace{0.125in}}
  \subfigure[Reduction at a join.]{
    \label{figure:join-reduce}
    \includegraphics[scale=0.25]{images/join-reduce-labels.eps}}
  \caption{Labeling scheme.}
\end{figure}

\subsubsection{Labeling at a join}

In the general case --- such as transition $m'$ in
Fig.\ref{figure:compatibility-fork-paths} --- a join is assigned the
union of the labels assigned to all its predecessors, as shown in
Fig.~\ref{figure:join-union}. But when the join receives all the
tokens starting from a particular fork --- such as transition $m$ in
Fig.\ref{figure:compatibility-fork-paths} --- the union is reduced to
remove the label elements indicating that fork, as shown in
Fig.~\ref{figure:join-reduce}. Usually, only a subset of the union is
reduced, since paths from unrelated forks may reach a join.

The reduction at the join ensures that the labeling scheme is
``closed'' --- all the extensions created within a fork region
disappear at the exit of a fork region\cite{ahir_thesis}. Finally, the
\sym{fin} transition is assigned an empty label.

\subsection{Label Representation Graph (LRG)}

The compatibility label is a record of every path reaching that
element from the $init$ transition, which results in an exponential
size. Comparing two labels for compatibility also has exponential
complexity, since every sequence in one label has to be compared with
every sequence in the other label.

We eliminate the complexity by using a {\em label representation
  graph} (LRG) as shown in Fig.~\ref{figure:LRG}. The LRG represents
labels as nodes, where edges represent the manner in which a label is
constructed from other labels. The LRG is a directed acyclic graph
with a single root node that represents the empty label.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.225]{images/LRG-example-cp.eps}
  \hspace{0.125in}
  \includegraphics[scale=0.225]{images/LRG-example.eps}
  \caption{Label Representation Graph.}
  \label{figure:LRG}
\end{figure}

Let $l(u)$ be the label represented by a node $u$. An edge $(u,v)$ in
the LRG may itself be labeled with a label element $e$, in which case
it represents the product operation $l(v) = l(u) * e$. If multiple
incoming edges are incident at a node $v$, then it represents a label
that is the union of all its predecessors. In a well-formed LRG,
multiple incoming edges are incident on a node if and only if they are
all unlabeled.

The LRG is a compact representation of compatibility labels in a
Type-2 petri-net. Each path reaching a node $u$ from the root of the
LRG represents one label sequence in the label $l(u)$. The following
result is proved in \cite{ahir_thesis}.

\begin{theorem}

  Two operations with labels represented by nodes $u$ and $v$ in the
  LRG are said to be compatible, if and only if one of the following
  is true:

\begin{enumerate}
  \item There is a path from $u$ to $v$ or \emph{vice versa}.
  \item There exists a node $a$ in the LRG, from which $u$ and $v$ are
        reachable along non-intersecting paths such that one of the
        following is true:
    \begin{enumerate}
      \item One or both paths begin with an unlabeled edge.
      \item The labels on the first edges in the paths indicate
            different forks.
    \end{enumerate}
\end{enumerate}
\label{theorem:compatibility-LRG}
\end{theorem}

\subsection{Shared operators}

We have used the LRG to implement arbiterless sharing in the AHIR
synthesis flow. We use a simple greedy algorithm to generate cliques
of pair-wise compatible operations that are mapped to a single
operator in the data-path. The incoming data-edges are routed through
multiplexers; the registers for the outgoing data-edges are not
shared.

This scheme for arbiterless sharing is quite effective in reducing
hardware costs, demonstrating support for scalable optimisations in
AHIR. Synthesis results show improvements in the throughput-area ratio
(measured in $\operatorname{Hz}/\operatorname{slice}$) in the range of
15--190\% depending on the application\cite{ahir_thesis}.

\section{The synthesis flow}

The synthesis flow uses the LLVM Compiler
Infrastructure\footnote{\url{http://llvm.org/}} to parse and optimise
the input C program. This is translated to an AHIR specification using
a CDFG as an intermediate step.

\subsection{C to CDFG}

\begin{figure}[!t]
\centering
\begin{subfloat}%
\hspace{0.125in}
  \begin{minipage}[b]{0.75in}%
    \small%
\begin{alltt}
d = m + n
b = m - n
if (b > 0):
  a = b + c
  d = e + a
x = d + 2
\end{alltt}%
  \end{minipage}%
  \caption{Pseudo-code.}%
  \label{figure:C-snippet}%
\hspace{0.125in}
\end{subfloat}%
\hspace{0.5in}%
\begin{subfloat}%
  \begin{minipage}[b]{0.85in}%
    \small%
\begin{alltt}
d1 = m + n
b = m - n
if (b > 0):
  a = b + c
  d2 = e + a
d3 = \(\phi\)(d1,d2)
x = d3 + 2
\end{alltt}%
  \end{minipage}%
  \caption{SSA form.}%
  \label{figure:ssa-snippet}%
\end{subfloat}%
\caption{A code fragment and its SSA form.}%
\label{figure:C-and-SSA}%
\end{figure}

The starting point is a C program.  In the current implementation,
the only restrictions on the C program are that the call graph
should not contain any cycles, and that function pointers should
not be used (pointers to data can be used in the usual way)\footnote{These restrictions
are not fundamental to the approach, but we have not gotten around to eliminating
them so far.}.
The C program is first converted to LLVM byte-code, which is based on
the SSA form\cite{muchnick}. This is a purely functional form that
removes the notion of individual {\it variables} from a program. Every
assignment to a variable is a distinct value; multiple assignments
that occur in branches are handled by a special instruction called the
$\phi$-function, as shown in Fig.~\ref{figure:C-and-SSA}.

\nocite{conditional_branches_rim_jain_1992}
\nocite{DFG_jong_1991}
\nocite{DFG_standard_eijndhoven_1992}
\nocite{flow_graph_orailoglu_1986}
\nocite{functional_synthesis_TASS_amellal_kaminska_1994}

The LLVM bytecode is then translated to a control data flow graph
(CDFG)\cite{conditional_branches_rim_jain_1992}-%
\cite{functional_synthesis_TASS_amellal_kaminska_1994}, as shown in
Fig.~\ref{fig:snippet-cdfg}. The CDFG represents instructions as
nodes connected by control and data flow edges. The edges in the CDFG
arise from three kinds of dependence in the original program:

\begin{itemize}
\item Data dependence that creates control and data flow.
\item Control structures that create control flow.
\item External dependence (in storage) that creates control flow.
\end{itemize}

\subsection{Generating AHIR from a CDFG}

\begin{figure*}
\centering
\subfigure[CDFG.]{%
  \includegraphics[scale=0.2]{images/cdfg.eps}%
  \label{fig:snippet-cdfg}}%
\hspace{0.25in}%
\subfigure[Control Path.]{%
  \includegraphics[scale=0.3]{images/controlpath.eps}%
  \label{fig:snippet-cp}}%
\hspace{0.25in}%
\subfigure[Data Path.]{%
  \includegraphics[scale=0.22]{images/datapath.eps}%
  \label{fig:snippet-dp}}%
\caption{Translating a CDFG to AHIR.}%
\label{figure:cdfg-to-ahir}%
\end{figure*}

The AHIR specification is generated by piece-wise translation. Each
node or edge in the CDFG is replaced by an equivalent \emph{AHIR
  fragment}, and the fragments are connected to obtain the control and
data paths. Fig.~\ref{figure:cdfg-to-ahir} shows the input CDFG and
the resulting AHIR specification for our example. Elements that are
obvious from the context have been hidden. Two structures are shown in
detail --- a decoder element (\texttt{D1}) that examines the condition
for a branch, and a multiplexer element (\texttt{P1}) that implements
a $\phi$-function.

\subsection{Correctness}

The method used by our synthesis flow guarantees that the generated
circuit specification is correct {\em by construction}. The first step
of translating a C program to a CDFG is a routine one that does not
need to be verified separately. 

The correctness of the second step (CDFG to AHIR) is shown
as follows\cite{ahir_thesis}: suppose that
$A$ was the AHIR specification generated from the
reference CDFG $G$.  Then every execution sequence
in $A$ is a member of the set of
possible execution sequences of $G$.  Now,
from $A$, we construct a new CDFG $G'$ such
that every execution sequence of  $G'$  is
an element of the set of execution sequences
of $A$.  Now, $G'$ and $G$ are shown to be isomorphic,
so that their sets of execution sequences are in 
a one-to-one correspondence with each other.  This
shows that the sets of execution sequences in $A$ and
$G$ are in a one-to-one correspondence.

\subsection{AHIR to VHDL}

The conversion of an AHIR network to VHDL is straightforward.
Currently, we produce a system which is synchronous and uses
a single clock (positive edges only).  Each symbol that is
exchanged between the control path and the data path is coded 
by a pulse which is sampled high by exactly one clock
edge.  Symbols are associated with transitions as indicated
earlier.

Places in the AHIR control path are handled as follows:
\begin{enumerate}
\item If a place has a single predecessor transition which is
an output transition and a single successor which is an input
transition, then it is optimized away.
\item If a place has a single successor transition which is
an output transition, and if this successor transition is
not a join, then the place is modeled by an OR gate whose inputs
are its predecessor transitions.
\item In the rest of the cases, the place is modeled as 
a finite state machine with two states, with the state being
set by any of its predecessors, and reset by any of its successors.
\end{enumerate}
The number of places that need to be modeled by state machines is
usually a small fraction of the total number of places.
Output transitions are modeled by AND gates, with each predecessor place
to the transition providing an input to the AND gate.  The cost
of implementing the control path is thus minimal in relation to
the implementation of the data path and memory subsystem.

Operators in the data path compute their results in a single
clock cycle, and the outputs are registered.  Shared operators
are constructed using multiplexers in a standard manner.
The instantiation of operators in the data path, 
and the interconnections within
the data path and between the data path and the control path
mirror those in the AHIR representation, except for the fact
that several operators in the AHIR data path can be mapped
to the same physical operator (as identified by arbiterless
operating sharing).
The inter module link layer is completed using the call graph
corresponding to the source code.

The memory sub-system is implemented as a multiple-bank (each
bank is a single-cycle synchronous SRAM)
pipe-lined memory which offers as many load/store ports as
there are load/store operators across the data paths.  The memory
sub-system is constructed using a request-complete protocol and
ensures that read/write operations complete in the order
that they were requested. The number of memory banks in the memory
subsystem,
and the degree of pipe-lining are parameters/generics that
can be selected based on the memory reference characteristics
of the application.

\section{Experimental Evaluation of the end-to-end synthesis flow}

We have implemented a complete C-to-ASIC tool-flow based on commercial
synthesis tools from Cadence and Synopsys.  We selected the
following set of applications for the experiments (the task used
to characterize the energy dissipation is also indicated).
\begin{enumerate}
\item A5/1, a simple stream cipher (basic task: produce one key bit).
\item AES, the standard AES-128 block cipher (basic task: encrypt a 128-bit block).
\item LPK, the standard Linpack benchmark (basic task: LU factorization of $100\times100$ matrix with
floating point entries).
\item FFT (basic task: 64 point floating point FFT).
\item RBT, the red-black tree data structure (basic task: insertion of $1000$ elements into
the tree).
\end{enumerate}

In each case, the VHDL generated by the C-to-RTL flow was synthesized
using Synposys Design Compiler, using the OSU Standard Cell
Library\footnote{\url{http://vcag.ecen.okstate.edu/projects/scells/}}
for the TSMC $0.18\mu$ technology. Memories were modeled using CACTI
SRAM
models\cite{CACTI}\footnote{\url{http://www.hpl.hp.com/research/cacti/}}.
The resulting net-list was mapped to layout using the Cadence SOC
Encounter tool. No human intervention was used in the entire process
from C to the final layout (other than specifying the operating
frequency to the automatic place-and-route tools). The total time
needed for the entire process (from C to layout) was of the order of a
few minutes for A5/1 to a few hours for the LPK case. A simulation
based toggle file was then used with the extracted net-list to
estimate power dissipation in each of the generated circuits. The
reciprocal of the energy used for completing a task is equivalent to
the throughput achieved for each watt of power supplied, commonly
termed as {\em performance per watt}

\begin{table}[htb]
  \centering
  \caption{Area/Delay/Power/Energy data for AHIR circuits in the TSMC $0.18\mu$ technology}
  \label{table:tsmc-180nm-data}
  \renewcommand\arraystretch{1.2}
  \setlength{\tabcolsep}{1ex}
  \begin{tabular}{c|c|c|c|c|c}
    \hline
    & Area & Freq & Delay & Power & Energy \\
    & (mm$^2$) & (MHz) & (ms) & (mW) & ($\mu$J) \\
    \hline
    \hline
    A5/1 & 1.45 & 71.4 & 0.28$\mu$s & 73 & 20.44\,nJ \\
    \hline
    AES & 6.5 & 71.4 & 0.428 & 338 & 144.7 \\
    \hline
    FFT & 5.1 & 41.7 & 0.155  & 95 & 14.7  \\
    \hline
    LPK & 27 & 41.7 & 37.7 & 273 & 10300  \\
    \hline
    RBT & 18 & 41.7 & 9.9 & 153 & 1511 \\
    \hline
  \end{tabular}

\end{table}


As a reference to judge the energy efficiency of the AHIR generated
circuits, we consider a popular low power processor (henceforth
denoted by $P$), namely the Intel Atom N270
processor\footnote{\url{http://www.intel.com/products/processor/atom/index.htm}},
which is built in Intel's $45nm$ technology. This processor is readily
available, combines high performance with low power dissipation, has
full floating point support, and is a part of many low power computing
devices. The sample set of C programs was run on the processor $P$
(exactly the same program used in the hardware generation was run on
the processor). In each case, the number of cycles needed to finish
one task (as described above) specific to that program was measured.
The data sets for the programs were small enough to fit into the
processor cache, and the cycle count was averaged across multiple
runs. These cycle counts were then used to find the delay and energy
consumption for the processor. All values for area, frequency and
power dissipation for the processor were obtained from the
corresponding data-sheet.

\begin{table}[htb]
  \centering
  \caption{Area/Delay/Power/Energy results for the processor $P$ built in a $45nm$ process}
  \label{table:atom-power-delay}
  \renewcommand\arraystretch{1.2}
  \setlength{\tabcolsep}{1ex}
  \begin{tabular}{c|c|c|c|c|c}
    \hline
    & Area & Freq & Delay & Power & Energy \\
    & (mm$^2$) & (MHz) & (ms) & (mW) & ($\mu$J) \\
    \hline
    \hline
    A5/1 & 25 & 1600 & 0.12$\mu$s & 2500 & 298.44\,nJ  \\
    \hline
    AES & 25 & 1600 & 0.036 & 2500 & 89.362 \\
    \hline
    FFT & 25 & 1600 & 0.022 & 2500 & 55.64  \\
    \hline
    LPK & 25 & 1600 & 7.90 & 2500 & 19740  \\
    \hline
    RBT & 25 & 1600 & 0.36 & 2500 & 891.89 \\
    \hline
  \end{tabular}

\end{table}

If we compare the results for the $180nm$ AHIR circuits with
the $45nm$ processor $P$, we find that except for the AES and
RBT cases,
the energy consumed per task by the AHIR circuit is appreciably
lower than that for the processor $P$.  This is significant
because the processor $P$ is implemented in a technology which
is four generations ahead of the AHIR circuits.  

Now, suppose
that  we scale the 
energy numbers assuming migration of the AHIR circuits
from $180nm$ to the $45nm$ technology node, using 
the following  scaling rules (assume that $S = 45/180 = 0.25$):
\begin{itemize}
\item The operating voltage is scaled from $1.8V$ to $0.9V$.
\item The internal capacitances are assumed to be scaled by $S$.
\item The operating frequency of the generated circuit is scaled
by $1/S$.
\item Half of the total power dissipation of the $45nm$ circuits
is due to leakage\cite{ref:Frank}, and the other half is due to switching power. 
Note that this is a conservative estimate, and the actual leakage is
likely to be lower at the power levels corresponding to the
AHIR circuits.
%
% 
The leakage power is a negligible fraction of the
total power dissipation in the $180nm$ technology.
\item The area is scaled by $S^2$.
\end{itemize}
Thus, the per task energy consumption due to switching
activity alone will scale by $1/16$, and since
we are assuming that leakage power dissipation is half the total,
the total per task energy consumption of the circuit will 
scale by $1/8$.
The area of the circuits will scale by $1/16$.  
The power dissipation will scale by $1/2$ (this would have been
$1/4$ without leakage).
This scaling, though approximate, can be justified since the 
area of the resulting circuits (in each case) is small, so 
that interconnect effects are not likely to be pronounced.

\begin{table}[htb]
  \centering
  \caption{Area/Delay/Power/Energy RATIOS (processor $P$ values relative to the scaled AHIR values)}
  \label{table:Ratios}
  \renewcommand\arraystretch{1.2}
  \setlength{\tabcolsep}{1ex}
  \begin{tabular}{c|c|c|c|c|c}
    \hline
    & Area & Freq & Delay & Power & Energy \\
    \hline
    \hline
    A5/1 & 275.8 & 5.6 & 1.7 & 68.5 & 116.6  \\
    \hline
    AES & 61.5 & 5.6 & 0.34 & 14.8 & 4.9 \\
    \hline
    FFT & 78.4 & 9.6 & 0.57 & 52.6 & 30.3  \\
    \hline
    LPK & 14.8 & 9.6 & 0.84 & 18.3 & 15.3  \\
    \hline
    RBT & 22.2 & 9.6 & 0.15 & 32.7 & 4.7 \\
    \hline
  \end{tabular}

\end{table}

In Table \ref{table:Ratios}, we tabulate the ratios of
the performance parameters area, frequency, delay, power
and energy for the processor $P$ relative to the scaled values
for the AHIR circuits assuming the scaling rules shown above.
From the table, we see that across all applications, the energy consumed
by AHIR circuits is considerably lower than the
processor (the energy per task for the processor is
between $4.7X$ and $116.6X$ that of the corresponding AHIR circuit).  
The improvement is highest in the case of
bit-level manipulation applications such as A5/1, and
lowest in the case of random ``pointer-chasing'' code.  Note
that the improvement in the
case of  floating point programs (FFT and LPK) is also
substantial.
The delays in completing the tasks are lower 
for the processor $P$ in almost all the cases.  This
is mainly due to the fact that currently, the operators used in 
the AHIR generated RTL are not pipelined or optimized in any
manner during the logic synthesis and physical
design. This limits the operating clock frequency of the circuits
synthesized from the AHIR generated RTL.  

\section{Conclusion}

We have presented a high-level synthesis flow that converts
an algorithm described in C into a VHDL description.  The
synthesis flow uses a factored control-data-storage internal representation
and is correct by construction.  On this internal
representation, optimizations such as arbiter-less operator sharing 
can be carried out in an efficient manner.  The flow can thus
be applied to large programs describing a wide variety of algorithms.
Our experiments demonstrate that across this variety of
algorithms, an ASIC
implemented using the RTL generated by this high-level
synthesis flow uses a significantly lower energy per task (or equivalently,
has higher performance per watt) than industry-standard low-power microprocessors. 
Thus, the range of algorithms
which can be handled, and the ease of obtaining an implementation
which is likely to be more energy efficient than a processor 
indicate that this high-level synthesis flow provides an 
alternative to embedded processors for implementing complex algorithms in hardware.

\bibliography{ref}
\bibliographystyle{IEEEtran}

\end{document}

% LocalWords:  Req Ack LL DP CP cmpgt br cdfg cp dp ir intra req init NCA SA mW
% LocalWords:  TPR STPR STPRs ccccc RTL ASIC petri ack versa liveness LRG LLVM
% LocalWords:  VHDL SRAM Synopsys AES LPK Linpack FFT RBT Synposys OSU TSMC nJ
% LocalWords:  nm
